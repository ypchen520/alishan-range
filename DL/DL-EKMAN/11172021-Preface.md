# Preface

* tasks that traditionally have been performed well only by humans
  * image classification
  * general language descriptions of images (image-captioning)
  * natural language translation
  * speech-to-text and text-to-speech conversion

## What is Deep Learning

* a class of machine learning algorithms that
  * use multiple layers of computational units where
    * each layer learns its own representation of the input data
      * These layers are combined by later layers in a hierarchical fashion
* AI
  * ML
    * DL
      * DNN

## Brief History of Deep Neural Networks

### First Wave

* the first model of an artificial neuron was introduced in 1943 (McCulloch and Pitts)
* 1957: Rosenblatt perceptron
* first AI winter: a book by Minsky and Papert (1969)
  * Olazaran (1996): misrepresented?
  * Schmidhuber (2015): learning algorithm for multilevel networks (Ivakhnenko and Lapa, 1965)

### Second Wave

* the 1980s
* popularized: the backpropagation algorithm for automatic training of multilayer networks (Rumelhart et al., 1986)
  * Linnainmaa, 1970
  * Werbos, 1981
* LeNet, 1989
  * convolutional neural network (CNN)
    * handwritten zip codes (LeCun et al., 1990)
    * built on Fukushima's Neocognitron (1980): the first published CNN
* fell out of fashion
  * the limited computational capability at the time
    * prevent the network from scaling to larger problems
  * other traditional machine learning approaches were perceived as better alternatives

### Third Wave

* came together in 2012
  * a combination of algorithmic progress
  * availability of massive datasets
  * the ability to use graphics processing units (GPUs) for general purpose computing
* popularized: AlexNet (Krizhevsky et al. 2012)
  * computer vision competition known as the ImageNet challenge
* the term deep networks: 2006
  * Graves and colleagues (2009) won competitions in handwriting recognition with a neural network
  * Ciresan and colleagues (2011) used a GPU accelerated network for image classification

### More detailed description of the history of DL: Schmidhuber's (2015) overview

## Is This Book For You?

* It is important to learn about traditional ML
* linear regression

## Is DL Dangerous

* a study of a commercially available facial recognition system (Buolamwini and Gebru, 2018)
  * skin color
  * the website of the Algorithmic Justice League
* fake pornography (Dickson, 2019)
* the risk of learning and even amplifying human biases
* the emergence of algorithmic auditing
  * researchers identify and report human biases (Raji and Buolamwini, 2019)
  * the released DL model (Mitchell et al., 2018)
  * data used to create such systems (Gebru et al., 2018)
* Thomas, 2019
  * a checklist of questions to guide DL practitioners throughout the course of a project to avoid ethical problems  

## Choosing a DL Framework

* A DL framework provides funtionality that handles much of the low-level details when implementing DL models
* Caffe
* Theano
* MXNet
* Torch
* TensorFlow
* PyTorch
* Keras
* TensorRT
* the most popular
  * TensorFlow
    * the Keras API
  * PyTorch
  * MXNet
* TensorFlow vs. PyTorch

## Prerequisites for Learning DL

### Statistics and probability theory

* variance
* standardize a random variable

### Linear Algebra

* a weighted sum of variables
* describe additions and multiplications *in a compact manner*
* vectors
* matrices
* dot products
* matrix-vector multiplications
* matrix-matrix multiplications

### Calculus

* the learning part of DL is based on
  * minimizing the value of a function known as a loss function or error function
* concepts from calculus
  * computing the derivative of a function of a single variable
  * computing partial derivatives of a function of multiple variables
  * calculating derivatives using the chain rule of calculus

### Numerical methods for constrained and unconstrained optimization

* gradient descent: an iterative method
  * finding extreme points in continuous functions

### Python programming

* NumPy (numerical Python)
* pandas (Python Data Manipulation Library)
  * manipulate multidimensional data
* plotting data with matplotlib

### Data representation

* highly optimized ML frameworks
  * input data needs to be converted into **suitable formats** that can be consumed by these frameworks
* the format of the data
  * images
    * RGB representation
  * text
    * how characters are represented by a computer
* raw input data needs to be cleaned
  * missing or duplicated data entries
  * timestamps from different time zones
  * typos originating from manual processing
  
## About the Code Examples

* DL algorithns are based on stochastic optimization techniques

## How to Read This Book

* summarize a group of related techniques once they have all been introduced

### Appendixes

* Appendix A: Chapter 3
* Appendix B: Chapter 8
* Appendix C: Chapter 13
* Appendix D: Chapter 15

### Guidance for readers who do not want to read all of this book

* Computer vision track
  * Appendix B
    * object detection
    * semantic segmentation
    * instance segmentation
  * skim Chapters 9 through 11 about 
    * recurrent neural networks (RNNs)
    * time series prediction
* Language processing track
  * skim Chapter 8
    * the description of skip connections
  * read Chapters 9 through 13
  * Appendix C
  * Chapters 14 amd 15
  * Appendix D
  * word embeddings
  * GPT and BERT

## Overview of Each Chapter and Appendix

### Chapter 1 - The Rosenblatt Perceptron

* perceptron
* limitations
* combine multiple perceptrons

### Chapter 2 - Gradient-Based Learning

* gradient descent
* stepping-stone for multilevel networks

### Chapter 3 - Sigmoid Neurons and Backpropagation

* backpropagation algorithm
  * automatic learning in DNNs
* binary classification

### Chapter 4 - Fully Connected Networks Applied to Multiclass Classification

* datasets
  * training set
  * test set
* a network's ability to generalize
* multiclass classification
* classifying handwritten digits

### Chapter 5 - Toward DL: Frameworks and Network Tweaks

* techniques for training deeper networks

### Chapter 6 - Fully Connected Networks Applied to Regression

* a regression problem
  * predict a numerical value

### Chapter 7 - Convolutional Neural Networks Applied to Image Classification

* the DL boom in 2012

### Chapter 8 - Deeper CNNs and Pretrained Models

* GoogLeNet
* VGG
* ResNet
  * pretrained ResNet

### Chapter 9 - Predicting Time Sequences with Recurrent Neural Networks

* handle data of different input lengths
* text and speech
  * sequences of varying lengths
* RNN
  * predict the next data point in a time series

### Chapter 10 - Long Short-Term Memory

* problems that prevent RNNs from learning long-term dependencies
* LSTM: better handling of long sequences

### Chapter 11 - Text Autocompletion with LSTM and Beam Search

* LSTM-based RNNs
* Beam search
* autocompletion of text
* natural language generation (NLG)

### Chapter 12 - Neural Language Models and Word Embeddings

* previous: individual characters instead of words
* work with words and their semantics
* vector space (embedding space)
  * relationships between words
* how to create word vectors in an embedding space
* Natural Language Understanding (NLU)
  * sentiment analysis

### Chapter 13 - Word Embeddings From word2vec and GloVe

* two popular techniques for creating word embeddings

### Chapter 14 - Sequence-To-Sequence Networks and Natural Language Translation

* sequence-to-sequence network
  * a combination of two RNNs
  * its output sequence cna be of a different length than the input sequence
* natural language translator
* encoder-decoder architecture

### Chapter 15 - Attention and the Transformer

* attention: improve the accuracy of encoder-decoder architectures
* neural machine translator
* the attention-based Transformer architecture
  * the key building blockin many NLP applications

### Chapter 16 - One-To-Many Network for Image Captioning

* textual descriptions of images
  * attention

### Chapter 17 - Medley of Additional Topics

* autoencoders
* multimodal learning
* multitask learning
* neural architecture search

### Chapter 18 - Summary and Next Steps

* ethical AI and data ethics

### Appendix A - Linear Regression and Linear Classifiers

* traditional ML techniques

### Appendix B - Object Detection and Segmentation

* techniques to detect and classify multiple objects in a single image
  * coarse-grained: bounding boxes
  * fine-grained: pinpoint the individual pixels

### Appendix C - Word Embeddings Beyond word2vec and GloVe

* techniques that can handle words that did not exist in the training dataset
* a technique that can handle cases in which a word has a different meaning depending on its context

### Appendix D - GPT, BERT, and RoBERTa

* architectures that build on the Transformer
  * improvements in NLP tasks

### Appendix E - Newton-Raphson Versus Gradient Descent

* Newton-Raphson vs. gradient descent

### Appendix F - Matrix Manipulation of Digit Classification Network

* optimized variations of a programming example implementing a neural network in Python code

### Appendix G - Relating Convolutional Layers to Mathematical Convolution

* a mathematical operation known as convolution

### Appendix H - Gated Recurrent Units

* a simplified version of LSTM: GRU

### Appendix I - Setting Up a Development Environment

* how to install deep learning framework and where to find the code examples
* key differences between TensorFlow and PyTorch

### Appendix J - Cheat Sheets
